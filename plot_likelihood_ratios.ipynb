{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Pre-test vs. post-test analysis\n",
        "\n",
        "Suppose we have a population of subjects with physiological measurements `X`\n",
        "that can hopefully serve as indirect bio-markers of the disease and actual\n",
        "disease indicators `y` (ground truth). Most of the people in the population do\n",
        "not carry the disease but a minority (in this case around 10%) does:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import make_classification\n",
        "\n",
        "X, y = make_classification(n_samples=10_000, weights=[0.9, 0.1], random_state=0)\n",
        "print(f\"Percentage of people carrying the disease: {100*y.mean():.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A machine learning model is built to diagnose if a person with some given\n",
        "physiological measurements is likely to carry the disease of interest. To\n",
        "evaluate the model, we need to assess its performance on a held-out test set:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then we can fit our diagnosis model and compute the positive likelihood\n",
        "ratio to evaluate the usefulness of this classifier as a disease diagnosis\n",
        "tool:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import class_likelihood_ratios\n",
        "\n",
        "estimator = LogisticRegression().fit(X_train, y_train)\n",
        "y_pred = estimator.predict(X_test)\n",
        "pos_LR, neg_LR = class_likelihood_ratios(y_test, y_pred)\n",
        "print(f\"LR+: {pos_LR:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Since the positive class likelihood ratio is much larger than 1.0, it means\n",
        "that the machine learning-based diagnosis tool is useful: the post-test odds\n",
        "that the condition is truly present given a positive test result are more than\n",
        "12 times larger than the pre-test odds.\n",
        "\n",
        "## Cross-validation of likelihood ratios\n",
        "\n",
        "We assess the variability of the measurements for the class likelihood ratios\n",
        "in some particular cases.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "def scoring(estimator, X, y):\n",
        "    y_pred = estimator.predict(X)\n",
        "    pos_lr, neg_lr = class_likelihood_ratios(y, y_pred, raise_warning=False)\n",
        "    return {\"positive_likelihood_ratio\": pos_lr, \"negative_likelihood_ratio\": neg_lr}\n",
        "\n",
        "\n",
        "def extract_score(cv_results):\n",
        "    lr = pd.DataFrame(\n",
        "        {\n",
        "            \"positive\": cv_results[\"test_positive_likelihood_ratio\"],\n",
        "            \"negative\": cv_results[\"test_negative_likelihood_ratio\"],\n",
        "        }\n",
        "    )\n",
        "    return lr.aggregate([\"mean\", \"std\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We first validate the :class:`~sklearn.linear_model.LogisticRegression` model\n",
        "with default hyperparameters as used in the previous section.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import cross_validate\n",
        "\n",
        "estimator = LogisticRegression()\n",
        "extract_score(cross_validate(estimator, X, y, scoring=scoring, cv=10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We confirm that the model is useful: the post-test odds are between 12 and 20\n",
        "times larger than the pre-test odds.\n",
        "\n",
        "On the contrary, let's consider a dummy model that will output random\n",
        "predictions with similar odds as the average disease prevalence in the\n",
        "training set:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.dummy import DummyClassifier\n",
        "\n",
        "estimator = DummyClassifier(strategy=\"stratified\", random_state=1234)\n",
        "extract_score(cross_validate(estimator, X, y, scoring=scoring, cv=10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here both class likelihood ratios are compatible with 1.0 which makes this\n",
        "classifier useless as a diagnostic tool to improve disease detection.\n",
        "\n",
        "Another option for the dummy model is to always predict the most frequent\n",
        "class, which in this case is \"no-disease\".\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "estimator = DummyClassifier(strategy=\"most_frequent\")\n",
        "extract_score(cross_validate(estimator, X, y, scoring=scoring, cv=10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The absence of positive predictions means there will be no true positives nor\n",
        "false positives, leading to an undefined `LR+` that by no means should be\n",
        "interpreted as an infinite `LR+` (the classifier perfectly identifying\n",
        "positive cases). In such situation the\n",
        ":func:`~sklearn.metrics.class_likelihood_ratios` function returns `nan` and\n",
        "raises a warning by default. Indeed, the value of `LR-` helps us discard this\n",
        "model.\n",
        "\n",
        "A similar scenario may arise when cross-validating highly imbalanced data with\n",
        "few samples: some folds will have no samples with the disease and therefore\n",
        "they will output no true positives nor false negatives when used for testing.\n",
        "Mathematically this leads to an infinite `LR+`, which should also not be\n",
        "interpreted as the model perfectly identifying positive cases. Such event\n",
        "leads to a higher variance of the estimated likelihood ratios, but can still\n",
        "be interpreted as an increment of the post-test odds of having the condition.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "estimator = LogisticRegression()\n",
        "X, y = make_classification(n_samples=300, weights=[0.9, 0.1], random_state=0)\n",
        "extract_score(cross_validate(estimator, X, y, scoring=scoring, cv=10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Invariance with respect to prevalence\n",
        "\n",
        "The likelihood ratios are independent of the disease prevalence and can be\n",
        "extrapolated between populations regardless of any possible class imbalance,\n",
        "**as long as the same model is applied to all of them**. Notice that in the\n",
        "plots below **the decision boundary is constant** (see\n",
        "`sphx_glr_auto_examples_svm_plot_separating_hyperplane_unbalanced.py` for\n",
        "a study of the boundary decision for unbalanced classes).\n",
        "\n",
        "Here we train a :class:`~sklearn.linear_model.LogisticRegression` base model\n",
        "on a case-control study with a prevalence of 50%. It is th\n",
        "n evaluated over\n",
        "populations with varying prevalence. We use the\n",
        ":func:`~sklearn.datasets.make_classification` function to ensure the\n",
        "data-generating process is always the same as shown in the plots below. The\n",
        "label `1` corresponds to the positive class \"disease\", whereas the label `0`\n",
        "stands for \"no-disease\".\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.inspection import DecisionBoundaryDisplay\n",
        "\n",
        "populations = defaultdict(list)\n",
        "common_params = {\n",
        "    \"n_samples\": 10_000,\n",
        "    \"n_features\": 2,\n",
        "    \"n_informative\": 2,\n",
        "    \"n_redundant\": 0,\n",
        "    \"random_state\": 0,\n",
        "}\n",
        "weights = np.linspace(0.1, 0.8, 6)\n",
        "weights = weights[::-1]\n",
        "\n",
        "# fit and evaluate base model on balanced classes\n",
        "X, y = make_classification(**common_params, weights=[0.5, 0.5])\n",
        "estimator = LogisticRegression().fit(X, y)\n",
        "lr_base = extract_score(cross_validate(estimator, X, y, scoring=scoring, cv=10))\n",
        "pos_lr_base, pos_lr_base_std = lr_base[\"positive\"].values\n",
        "neg_lr_base, neg_lr_base_std = lr_base[\"negative\"].values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will now show the decision boundary for each level of prevalence. Note that\n",
        "we only plot a subset of the original data to better assess the linear model\n",
        "decision boundary.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig, axs = plt.subplots(nrows=3, ncols=2, figsize=(15, 12))\n",
        "\n",
        "for ax, (n, weight) in zip(axs.ravel(), enumerate(weights)):\n",
        "    X, y = make_classification(\n",
        "        **common_params,\n",
        "        weights=[weight, 1 - weight],\n",
        "    )\n",
        "    prevalence = y.mean()\n",
        "    populations[\"prevalence\"].append(prevalence)\n",
        "    populations[\"X\"].append(X)\n",
        "    populations[\"y\"].append(y)\n",
        "\n",
        "    # down-sample for plotting\n",
        "    rng = np.random.RandomState(1)\n",
        "    plot_indices = rng.choice(np.arange(X.shape[0]), size=500, replace=True)\n",
        "    X_plot, y_plot = X[plot_indices], y[plot_indices]\n",
        "\n",
        "    # plot fixed decision boundary of base model with varying prevalence\n",
        "    disp = DecisionBoundaryDisplay.from_estimator(\n",
        "        estimator,\n",
        "        X_plot,\n",
        "        response_method=\"predict\",\n",
        "        alpha=0.5,\n",
        "        ax=ax,\n",
        "    )\n",
        "    scatter = disp.ax_.scatter(X_plot[:, 0], X_plot[:, 1], c=y_plot, edgecolor=\"k\")\n",
        "    disp.ax_.set_title(f\"prevalence = {y_plot.mean():.2f}\")\n",
        "    disp.ax_.legend(*scatter.legend_elements())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We define a function for bootstrapping.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def scoring_on_bootstrap(estimator, X, y, rng, n_bootstrap=100):\n",
        "    results_for_prevalence = defaultdict(list)\n",
        "    for _ in range(n_bootstrap):\n",
        "        bootstrap_indices = rng.choice(\n",
        "            np.arange(X.shape[0]), size=X.shape[0], replace=True\n",
        "        )\n",
        "        for key, value in scoring(\n",
        "            estimator, X[bootstrap_indices], y[bootstrap_indices]\n",
        "        ).items():\n",
        "            results_for_prevalence[key].append(value)\n",
        "    return pd.DataFrame(results_for_prevalence)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We score the base model for each prevalence using bootstrapping.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "results = defaultdict(list)\n",
        "n_bootstrap = 100\n",
        "rng = np.random.default_rng(seed=0)\n",
        "\n",
        "for prevalence, X, y in zip(\n",
        "    populations[\"prevalence\"], populations[\"X\"], populations[\"y\"]\n",
        "):\n",
        "    results_for_prevalence = scoring_on_bootstrap(\n",
        "        estimator, X, y, rng, n_bootstrap=n_bootstrap\n",
        "    )\n",
        "    results[\"prevalence\"].append(prevalence)\n",
        "    results[\"metrics\"].append(\n",
        "        results_for_prevalence.aggregate([\"mean\", \"std\"]).unstack()\n",
        "    )\n",
        "\n",
        "results = pd.DataFrame(results[\"metrics\"], index=results[\"prevalence\"])\n",
        "results.index.name = \"prevalence\"\n",
        "results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the plots below we observe that the class likelihood ratios re-computed\n",
        "with different prevalences are indeed constant within one standard deviation\n",
        "of those computed with on balanced classes.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(15, 6))\n",
        "results[\"positive_likelihood_ratio\"][\"mean\"].plot(\n",
        "    ax=ax1, color=\"r\", label=\"extrapolation through populations\"\n",
        ")\n",
        "ax1.axhline(y=pos_lr_base + pos_lr_base_std, color=\"r\", linestyle=\"--\")\n",
        "ax1.axhline(\n",
        "    y=pos_lr_base - pos_lr_base_std,\n",
        "    color=\"r\",\n",
        "    linestyle=\"--\",\n",
        "    label=\"base model confidence band\",\n",
        ")\n",
        "ax1.fill_between(\n",
        "    results.index,\n",
        "    results[\"positive_likelihood_ratio\"][\"mean\"]\n",
        "    - results[\"positive_likelihood_ratio\"][\"std\"],\n",
        "    results[\"positive_likelihood_ratio\"][\"mean\"]\n",
        "    + results[\"positive_likelihood_ratio\"][\"std\"],\n",
        "    color=\"r\",\n",
        "    alpha=0.3,\n",
        ")\n",
        "ax1.set(\n",
        "    title=\"Positive likelihood ratio\",\n",
        "    ylabel=\"LR+\",\n",
        "    ylim=[0, 5],\n",
        ")\n",
        "ax1.legend(loc=\"lower right\")\n",
        "\n",
        "ax2 = results[\"negative_likelihood_ratio\"][\"mean\"].plot(\n",
        "    ax=ax2, color=\"b\", label=\"extrapolation through populations\"\n",
        ")\n",
        "ax2.axhline(y=neg_lr_base + neg_lr_base_std, color=\"b\", linestyle=\"--\")\n",
        "ax2.axhline(\n",
        "    y=neg_lr_base - neg_lr_base_std,\n",
        "    color=\"b\",\n",
        "    linestyle=\"--\",\n",
        "    label=\"base model confidence band\",\n",
        ")\n",
        "ax2.fill_between(\n",
        "    results.index,\n",
        "    results[\"negative_likelihood_ratio\"][\"mean\"]\n",
        "    - results[\"negative_likelihood_ratio\"][\"std\"],\n",
        "    results[\"negative_likelihood_ratio\"][\"mean\"]\n",
        "    + results[\"negative_likelihood_ratio\"][\"std\"],\n",
        "    color=\"b\",\n",
        "    alpha=0.3,\n",
        ")\n",
        "ax2.set(\n",
        "    title=\"Negative likelihood ratio\",\n",
        "    ylabel=\"LR-\",\n",
        "    ylim=[0, 0.5],\n",
        ")\n",
        "ax2.legend(loc=\"lower right\")\n",
        "\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
